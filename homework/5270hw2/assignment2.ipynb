{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ELs-DoWgo5S9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "368f444540020bf0a7d7942f0551023c",
     "grade": false,
     "grade_id": "cell-211ab281d2664aea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 2: Text mining\n",
    "\n",
    "This assignment will teach you how to use text data to build predictive models. Before starting this assignment, make sure you read the Trans-American Airlines case study. The data for the case study is in the file \"tweets.csv\".\n",
    "\n",
    "## Grade Breakdown\n",
    "\n",
    "The grade breakdown for this assignment is as follows:\n",
    "1. **Questions & Code (80\\%):** Most of this assignment consists of completing short snippets of code and answering questions within the Jupyter notebook. Questions do not have partial credits. You either get all the points or you don't get any, so be careful in your responses.\n",
    "2. **Peer evaluation (20\\%):** You and your group members must evaluate each other by completing the peer evaluation (the link is in the assignment description in Canvas). **IMPORTANT**: You will receive no credits if you do not complete your peer evaluation as part of your submission, so please be careful. One of the questions in the evaluation is this one: `Did this group member helped you submit a better assignment or in less time than what you could have done on your own?` Your grade will depend on the answer of other group members, and their grade will depend on your answer. These are the possible answers:\n",
    "   * Great: \"Definitely. My assignment is much better or it took me much less time than if I had done it without them.\" (+10% to grade, or +20% if you are in a group of 2)\n",
    "   * Acceptable: \"To some extent. My assignment is slightly better or it took me slightly less time than if I had done it without them.\" (+5% to grade, or +10% if you are in a group of 2)\n",
    "   * Worrisome: \"Not really. They did not save me time or help me submit a better assignment, but they gave it an honest try.\" (+2% to grade, or +4% if you are in a group of 2, and the person who answered this should reach out to the corresponding group member)\n",
    "   * Unacceptable: \"No. And they offered me very little help or no help at all.\" (+0%, the person who answered this should reach out to the corresponding group member, and the professor will look into it)\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "Before you answer the questions below, let's first load the data that was labeled by your assistant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "deletable": false,
    "editable": false,
    "id": "TqQm2LsNo1Y_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b66805b62a7e7f516dfcd3be379345f",
     "grade": false,
     "grade_id": "cell-8ac8bb4b987eed3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e653bc6d-e750-4544-dbce-e5f332978900"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rL27hdWe2WqT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaa309a376ed10ff0009cd113a030f6e",
     "grade": false,
     "grade_id": "cell-4cefcf57ce9954ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1: Target Variable (1 point)\n",
    "\n",
    "Code a function that takes the whole data set as an input and returns a binary target variable that we should predict to address the problem discussed in the case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1b9e9a2f9f1c60fe6ce8d76a61ece8",
     "grade": false,
     "grade_id": "cell-7e1d1b1bd5d588c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_target_variable(data):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d93ee8313ccaa761eefb0c276f55e97",
     "grade": false,
     "grade_id": "cell-1373ea3914205389",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The output of your function should be a pandas `Series` where values are either `True` or `False`. A `True` value should represent the outcome of interest (e.g., the customer is angry). Check that your function is giving the correct type of output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb47804c1b6452caaac91a1247f260ed",
     "grade": true,
     "grade_id": "cell-6c6d1be21f2557d7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y = get_target_variable(df)\n",
    "assert type(y) == pd.core.series.Series\n",
    "assert len(y) == len(df)\n",
    "assert y.dtype == bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e91b6c8c1cb0c10646102eb578759bc0",
     "grade": false,
     "grade_id": "cell-804930b7270cf0fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2: Building a Predictive Model (1 point)\n",
    "\n",
    "The following code splits the data into a training set (13,640 tweets) and a holdout set (1,000 tweets). It then transforms the text of each tweet using the bag-of-words technique discussed in Chapter 10. Each possible word that could appear in a tweet is represented as a binary feature that takes a value of 1 if the word is present in the tweet and a value of 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "deletable": false,
    "editable": false,
    "id": "aU0kGvok1-oN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58a439fa4f956ae96d8e72f8545e24f",
     "grade": false,
     "grade_id": "cell-213504f5de2f9a16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "15ed3de9-cd2c-490a-f938-3f3aef77bb8e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_data = df['text']\n",
    "text_train, text_holdout, y_train, y_holdout = train_test_split(text_data, y, test_size=1000, random_state=0)\n",
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "binary_vectorizer.fit(text_train)\n",
    "X_binary_train = binary_vectorizer.transform(text_train)\n",
    "X_binary_holdout = binary_vectorizer.transform(text_holdout)\n",
    "X_binary_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc49b9f99b58347bafaddcfc4a9c53ab",
     "grade": false,
     "grade_id": "cell-b0dac1218c040f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the matrix that results from transforming the text in the training data consists of 13,640 rows (tweets) and 14,436 features (words)! The output above also shows that the data is being stored in a sparse matrix (as opposed to the typical dense matrix). Given the shape of the matrix, this means there are \\~197 million cells that should have values. However, from the above, we can see that only \\~218k cells (\\~0.1% of the cells) have values! Why is this?\n",
    "\n",
    "To save space, sklearn uses a sparse matrix. This means that only values that are not zero are stored, which saves a ton of memory and makes the computation of models much more efficient!\n",
    "\n",
    "**Code a function that returns a logistic regression model that is trained and tuned using the training data**. Use `GridSearchCV` with 10 folds to tune the model according to AUC. Use the parameters `solver=\"liblinear\"` and `random_state=42` for the logistic regression. Also, try the following values for the `C` parameter with the cross-validation: 0.01, 0.1, 1, 10. For more information on these parameters, [check out the documentation on logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "*Side note (and practical tip):* When you just want to do a \"quick\" tune of a regularization parameter (like `C`), it's a common practice to try powers of 10 (e.g., 0.01, 0.1, 1, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bcb3e954b6ffb40d5d5c952fd65f35d",
     "grade": false,
     "grade_id": "cell-0b7e89ed9df4b059",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_model(X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49ef4db07225bcd0fb39dc3acb8b521a",
     "grade": false,
     "grade_id": "cell-e8db2c5123178a49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check that your function is giving the correct type of output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9712b7ac41c476a52d2ee87fdfd8220",
     "grade": true,
     "grade_id": "cell-5abc1d10340a17cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_model = get_model(X_binary_train, y_train)\n",
    "assert type(my_model) == LogisticRegression \n",
    "assert len(my_model.predict(X_binary_train)) == len(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff0df9fa86ca28372ba193dcd77136c4",
     "grade": false,
     "grade_id": "cell-5c765a9001418370",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3: Most Predictive Words (0.5 points)\n",
    "\n",
    "The code below shows the features with the largest coefficients in your model. Use it to show the most predictive words. Pick a few words that catch your attention (at least 2 or 3). Why do you think these words are predictive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf9f5092bbc0ef7410cbd418a5faa362",
     "grade": false,
     "grade_id": "cell-fdb6e2b537e41af7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=15):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"Coefficient size\")\n",
    "    plt.show()\n",
    "    \n",
    "feature_names = binary_vectorizer.get_feature_names()\n",
    "plot_coefficients(my_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b38b80c41f9cc5a35d69b8c29c1e4e6b",
     "grade": true,
     "grade_id": "cell-b21fbdc2c228f448",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "208396ebe8cffce262b536dad5caf90c",
     "grade": false,
     "grade_id": "cell-9c2af208bcffc0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4: Text Mining Limitations (0.5 points)\n",
    "\n",
    "For the purposes of this question, suppose we use a threshold of 0.5 to predict the target variable. The following code prints the text of five false positives and five false negatives in the validation set. Why do you think the model made these mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b05a8b62af14e6333b190938af0b04d",
     "grade": false,
     "grade_id": "cell-5652b10ae2c6d1cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = my_model.predict(X_binary_holdout)\n",
    "false_positives = text_holdout[(predictions == True) & (y_holdout == False)]\n",
    "false_negatives = text_holdout[(predictions == False) & (y_holdout == True)]\n",
    "print(\"===== FALSE POSITIVES\")\n",
    "print(false_positives.head(5).values)\n",
    "print(\"===== FALSE NEGATIVES\")\n",
    "print(false_negatives.head(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30b14022ef2057c52ffc807161dfe986",
     "grade": true,
     "grade_id": "cell-00947b63f3daa4d3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98883675e445db7cc598a686cd3b21a3",
     "grade": false,
     "grade_id": "cell-0043b2f00da655d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5: Model Evaluation (2 points)\n",
    "\n",
    "Based on the case study information, choose one of the following measures to evaluate your model:\n",
    "* Accuracy\n",
    "* Expected Value\n",
    "* AUC\n",
    "* Precision\n",
    "* Recall\n",
    "\n",
    "Then, code a function called `get_evaluation` that evaluates the performance of your model according to this measure. This function should receive two parameters: the labeled data in the holdout set and the probability predictions made by your model in the holdout set. To obtain full marks, you must choose the most appropriate evaluation measure and code it correctly. You are only allowed to use standard Python, scikit-learn, pandas, or numpy for this task.\n",
    "\n",
    "Print the model's performance. Do you think this performance is acceptable from a business perspective? Justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2aa9ec8690febe5ff64ff0beb209af0",
     "grade": false,
     "grade_id": "cell-b51f77262154d15f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_evaluation(y, probs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your function is giving the right output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78dec6d094c6ddbe0bd49d5efa434ca",
     "grade": true,
     "grade_id": "cell-485ca65f50391f29",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "probabilities = my_model.predict_proba(X_binary_holdout)[:, 1]\n",
    "result = get_evaluation(y_holdout, probabilities)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78b920ca45a6a123d6c4be07f84017b6",
     "grade": true,
     "grade_id": "cell-4a353f5bb495b759",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fdd0c3903d155affe033cf7e2a67c25",
     "grade": false,
     "grade_id": "cell-f6414006fdce5cf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 6: Using the model (0.5 point)\n",
    "\n",
    "Print the top 20 tweets in the holdout set with the highest probability of having a positive value for the target variable. What seems to be the main problem that Trans-American Airlines is facing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7002a2e34090a5a5a2f1f241f951d86",
     "grade": true,
     "grade_id": "cell-ab7ab4312b3a6ad9",
     "locked": false,
     "points": 0.2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85b854081feb0603df6390621eb0fca3",
     "grade": true,
     "grade_id": "cell-b22458d2b8cd6392",
     "locked": false,
     "points": 0.3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e657b55db6af25c4c62fa98930a7d2d",
     "grade": false,
     "grade_id": "cell-a7a7c6d55d4248fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 7: Conduct Benchmark (2 points)\n",
    "\n",
    "There are several options for transforming text into features besides using 1s and 0s to represent the presence or absence of a word. For example you can use integers to indicate how many times words appear; the term frequency - inverse document frequency (tf-idf) measure is another popular alternative (see Chapter 10 of the book). The code below shows how to transform text into features using these two other approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0e466667df52608d544cd725a82682d",
     "grade": false,
     "grade_id": "cell-54c76f0b996d8484",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# transform text to word counts\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(text_train)\n",
    "X_count_train = count_vectorizer.transform(text_train)\n",
    "\n",
    "# transform text to tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(text_train)\n",
    "X_tfidf_train = tfidf_vectorizer.transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3f32c20b7629287bf0604ef3776d661",
     "grade": false,
     "grade_id": "cell-befeb6cc5f3602cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Code a function that benchmarks the following approaches to transform text data into features:\n",
    "* `CountVectorizer(binary=True)`\n",
    "* `CountVectorizer()`\n",
    "* `TfidfVectorizer()`\n",
    "\n",
    "The function must:\n",
    "* Learn and tune models using the function `get_model` you coded for Question 2. As before, use the training data to train and tune models.\n",
    "* Evaluate the models using the function `get_evaluation` you coded for Question 5. As before, use the test data to evaluate the models.\n",
    "* Return a list with the evaluation performance of the three methods. The first element should correspond to `CountVectorizer(binary=True)`, the second element to `CountVectorizer()`, and the third element to `TfidfVectorizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b89db8460be2c53fd02e6a2d612b9c7",
     "grade": false,
     "grade_id": "cell-4939fbc8b308fd40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def conduct_benchmark(text_train, text_val, y_train, y_val):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaa4ae23f657ed1bca09fcac218da100",
     "grade": false,
     "grade_id": "cell-e00a27373b2da974",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check that your function is returning a list with 3 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2a60bb8795b0c315b33946dbc99afc",
     "grade": true,
     "grade_id": "cell-bb8bd29be0a354f0",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "benchmark = conduct_benchmark(text_train, text_holdout, y_train, y_holdout)\n",
    "assert len(benchmark) == 3\n",
    "print(benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e7152423e3691a1584a93879433bae4",
     "grade": false,
     "grade_id": "cell-9c4cd16f7ec38019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 8: Interpret Benchmark (0.5 points)\n",
    "\n",
    "Take a look at the benchmark results. Do you think the difference in performance is large or small? Why do you think the difference in performance is large/small? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53dd566e3a34916a8655e0c3614cf9bd",
     "grade": false,
     "grade_id": "cell-0524308e9623674f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Approach\":['binary', 'count', 'tfidf'], \"Performance\":benchmark})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c766030d1f96390d5f3c1dcbeb653fcd",
     "grade": true,
     "grade_id": "cell-accf504060b2cc55",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
